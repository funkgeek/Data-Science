{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portland Data Science Group\n",
    "## Sentiment analysis (five week series)\n",
    "### Week 2\n",
    "In this week's session, I decide to start digging into the natural language processing (NLP) part of this project. Because this is my first NLP project, in order to keep my goal realistic, I am going to build a very simple, Multinomial Naive Bayes model.\n",
    "\n",
    "Multinomial Naive Bayes models (https://en.wikipedia.org/wiki/Naive_Bayes_classifier) are quite frequently used as the first attempt when dealing with multinomial classification problems. By classification, it means the output y values are labels (e.g., quality grade of a product) instead of constinuous values (e.g., ratings in this dataset). Therefore, data preprocessing is a necessary part before feeding data to the model. \n",
    "\n",
    "The analysis in is notebook has followed a posted blog on sentiment analysis (Sentiment analysis for Yelp review classification https://medium.com/tensorist/classifying-yelp-reviews-using-nltk-and-scikit-learn-c58e71e962d9) and some codes are borrowed directly from the blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>gameID</th>\n",
       "      <th>rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172640</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Good:  Unique take on the hidden role games. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86674</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>A neat social deduction game with multiple tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10643</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Good hidden roles werewolf style game that can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31171</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Overall I hate Mafia/Werewolf, but this versio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165608</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Fun social deduction exercise that gets merrie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  gameID  rating                                            comment\n",
       "0  172640   24068     7.0  Good:  Unique take on the hidden role games. T...\n",
       "1   86674   24068     7.0  A neat social deduction game with multiple tea...\n",
       "2   10643   24068     7.0  Good hidden roles werewolf style game that can...\n",
       "3   31171   24068     7.0  Overall I hate Mafia/Werewolf, but this versio...\n",
       "4  165608   24068     7.0  Fun social deduction exercise that gets merrie..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data, tryout on the sample data\n",
    "import pandas as pd\n",
    "reviews = pd.read_csv('Data/boardgame-comments-english.csv', low_memory=False)\n",
    "reviews.rename(columns = {\"Compiled from boardgamegeek.com by Matt Borthwick\":'userID'}, inplace=True)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful libraries\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start the processing the text content. For each comment, we are going to break it into individual words and use these words later as the model input (i.e., the X matrix). There are various ways in doing so including the \"Gap\" module introduced by Andrew Ferlitsch (https://github.com/andrewferlitsch/Gap). But for now I'm going to use the conventional manual way to keep things short and simple (not necessary for the coding part but in terms of using fewer libraries and word/tag stuff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1: \"manually\" data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>gameID</th>\n",
       "      <th>rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172640</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>good  unique take on the hidden role games the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86674</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>a neat social deduction game with multiple tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10643</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>good hidden roles werewolf style game that can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31171</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>overall i hate mafiawerewolf but this version ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165608</td>\n",
       "      <td>24068</td>\n",
       "      <td>7.0</td>\n",
       "      <td>fun social deduction exercise that gets merrie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  gameID  rating                                            comment\n",
       "0  172640   24068     7.0  good  unique take on the hidden role games the...\n",
       "1   86674   24068     7.0  a neat social deduction game with multiple tea...\n",
       "2   10643   24068     7.0  good hidden roles werewolf style game that can...\n",
       "3   31171   24068     7.0  overall i hate mafiawerewolf but this version ...\n",
       "4  165608   24068     7.0  fun social deduction exercise that gets merrie..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all punctuation\n",
    "reviews['comment'] = reviews['comment'].str.replace('[^\\w\\s]','')\n",
    "# Lowercase all words\n",
    "reviews['comment'] = reviews['comment'].str.lower()\n",
    "# # Remove all stopwords\n",
    "# reviews['comment'] = reviews['comment'].apply(lambda x: [item for item in x.split() if item not in stopwords.words('english')])\n",
    "# This above line takes a long time to run (>36 min then I terminated the running), so I do it in the CountVectorizar part instead. \n",
    "# Why it takes so long to run? I have no ideas...\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "I use sklearn CountVectorizar (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to convert the comment to a matrix of token counts. This is essentially to create a sparse matrix as the X matrix input to our later Naive Bayes model. I use the nltk stopwords list as the reference for stopword removal. And if a word appears in >90% or <0.1% of all comments, I'll remove it from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(stop_words=stopwords.words('english'), max_df=0.9, min_df=0.001).fit(reviews['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2212"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of the vocabulary list (i.e., all the tokens considered)\n",
    "len(bow_transformer.vocabulary_ )\n",
    "# Actually not many tokens to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original comment is:\n",
      " played twice over the weekend still working out some of the rules  each time had a real blast hit the sweet spot with everyone  coop game with traitor element  very thematic and you dont have to be a fan of the series to enjoy it  31082014 play this for the experience   starstarstarstarnostar easy to learn starstarstarstarnostar fun factor starstarhalfstarnostarnostar   replayability  why this game stays in my collection the mechanics with the voting on decisions can feel repetitive and some turns you can find yourself with very little to do but the whole traitor role mechanic with other players possibly being a cylon or sympathiser can make this one of the most socially orientated and entertaining games \n",
      "After transformation, our input becomes:\n",
      "   (0, 237)\t1\n",
      "  (0, 375)\t1\n",
      "  (0, 439)\t1\n",
      "  (0, 505)\t1\n",
      "  (0, 567)\t1\n",
      "  (0, 604)\t1\n",
      "  (0, 624)\t1\n",
      "  (0, 644)\t1\n",
      "  (0, 651)\t1\n",
      "  (0, 677)\t1\n",
      "  (0, 699)\t1\n",
      "  (0, 716)\t1\n",
      "  (0, 727)\t1\n",
      "  (0, 747)\t1\n",
      "  (0, 772)\t1\n",
      "  (0, 827)\t1\n",
      "  (0, 832)\t2\n",
      "  (0, 836)\t1\n",
      "  (0, 948)\t1\n",
      "  (0, 1126)\t1\n",
      "  (0, 1159)\t1\n",
      "  (0, 1202)\t1\n",
      "  (0, 1239)\t1\n",
      "  (0, 1242)\t1\n",
      "  (0, 1375)\t1\n",
      "  (0, 1470)\t1\n",
      "  (0, 1472)\t1\n",
      "  (0, 1474)\t1\n",
      "  (0, 1497)\t1\n",
      "  (0, 1589)\t1\n",
      "  (0, 1620)\t1\n",
      "  (0, 1624)\t1\n",
      "  (0, 1661)\t1\n",
      "  (0, 1681)\t1\n",
      "  (0, 1734)\t1\n",
      "  (0, 1834)\t1\n",
      "  (0, 1852)\t1\n",
      "  (0, 1858)\t1\n",
      "  (0, 1915)\t1\n",
      "  (0, 1963)\t1\n",
      "  (0, 1995)\t1\n",
      "  (0, 2030)\t2\n",
      "  (0, 2049)\t1\n",
      "  (0, 2050)\t1\n",
      "  (0, 2155)\t1\n",
      "  (0, 2183)\t1\n",
      "blast\n",
      "collection\n"
     ]
    }
   ],
   "source": [
    "# Let's check a single comment to understand what's going on under the hood\n",
    "review_17 = reviews['comment'][16]\n",
    "print('The original comment is:\\n', review_17)\n",
    "\n",
    "bow_17 = bow_transformer.transform([review_17])\n",
    "print('After transformation, our input becomes:\\n', bow_17)\n",
    "\n",
    "# Let's see some words represented by these columns\n",
    "print(bow_transformer.get_feature_names()[237])\n",
    "print(bow_transformer.get_feature_names()[375])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can use this vectorizor to transform our comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (841645, 2212)\n",
      "Amount of Non-Zero occurrences:  13327632\n",
      "Density: 0.7158778452216686\n"
     ]
    }
   ],
   "source": [
    "X = bow_transformer.transform(reviews['comment'])\n",
    "# Check the shape of X\n",
    "print('Shape of Sparse Matrix: ', X.shape)\n",
    "print('Amount of Non-Zero occurrences: ', X.nnz)\n",
    "# Percentage of non-zero values\n",
    "density = (100.0 * X.nnz / (X.shape[0] * X.shape[1]))\n",
    "print('Density: {}'.format((density)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing on y values\n",
    "Multinomial Naive Bayes only works for categorical labels, not for continuous scores like the ratings in our data. In order to make our simple model to work, we need to round the rating in our data so that we have a limited number of labels (because it does not make too much sense to have a label say '7.1111'). In our dataset, there are a considerable amount of ratings are in the .5 scales. I feel it maybe quite biased to directly round such numbers up as normaly done. So I decide to create our \"rating labels\" based on .5 increase with a function. This function is directly borrowed from https://stackoverflow.com/questions/24838629/round-off-float-to-nearest-0-5-in-python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_of_rating(number):\n",
    "    \"\"\"\n",
    "    Round a number to the closest half integer.\n",
    "    >>> round_of_rating(1.3)\n",
    "    1.5\n",
    "    >>> round_of_rating(2.6)\n",
    "    2.5\n",
    "    >>> round_of_rating(3.0)\n",
    "    3.0\n",
    "    >>> round_of_rating(4.1)\n",
    "    4.0\n",
    "    \"\"\"\n",
    "\n",
    "    return round(number * 2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y values:  (841645,)\n"
     ]
    }
   ],
   "source": [
    "y = reviews['rating'].apply(round_of_rating)\n",
    "print('Shape of y values: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, after some experimentation, I found the Naive Bayes model only intakes integer lables, meaning it raises an error when the label is, for example 7.5. So we have to further modify the y values back to integers. One way to walk around this issue is to multiply the y values by two and then divide them by two after prediction. Actually, this is only for the purpose of intepretation. For model accuracy check, we don't even need to divide the ratings afterwards because ultimately we are only checking if the labels are predicted correctly or not. It's not super critical what the labels are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         14\n",
       "1         14\n",
       "2         14\n",
       "3         14\n",
       "4         14\n",
       "5         20\n",
       "6         20\n",
       "7         20\n",
       "8         20\n",
       "9         20\n",
       "10        17\n",
       "11        17\n",
       "12        17\n",
       "13        17\n",
       "14        17\n",
       "15        17\n",
       "16        17\n",
       "17        17\n",
       "18        17\n",
       "19        17\n",
       "20        17\n",
       "21        17\n",
       "22        17\n",
       "23        17\n",
       "24        17\n",
       "25        17\n",
       "26        17\n",
       "27        17\n",
       "28        17\n",
       "29        17\n",
       "          ..\n",
       "841615    14\n",
       "841616    14\n",
       "841617    14\n",
       "841618    14\n",
       "841619    14\n",
       "841620    14\n",
       "841621    14\n",
       "841622    14\n",
       "841623    14\n",
       "841624    14\n",
       "841625    14\n",
       "841626    14\n",
       "841627    14\n",
       "841628    14\n",
       "841629    14\n",
       "841630    14\n",
       "841631    15\n",
       "841632    15\n",
       "841633    15\n",
       "841634    15\n",
       "841635    15\n",
       "841636    15\n",
       "841637    15\n",
       "841638    15\n",
       "841639    15\n",
       "841640    15\n",
       "841641    15\n",
       "841642    15\n",
       "841643    15\n",
       "841644    15\n",
       "Name: rating, Length: 841645, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y *= 2\n",
    "y = y.astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes model\n",
    "After data preprocessing, the model construction, training, and testing process is actually pretty standard and straightforward. Again we split our entire dataset into train and dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nb.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "          2       0.14      0.20      0.17      1184\n",
      "          3       0.01      0.01      0.01        82\n",
      "          4       0.12      0.11      0.12      2489\n",
      "          5       0.00      0.00      0.00       202\n",
      "          6       0.13      0.10      0.11      4812\n",
      "          7       0.01      0.00      0.00       399\n",
      "          8       0.14      0.07      0.10      8611\n",
      "          9       0.04      0.01      0.02       746\n",
      "         10       0.19      0.10      0.13     15305\n",
      "         11       0.03      0.00      0.01      2262\n",
      "         12       0.24      0.21      0.22     31501\n",
      "         13       0.25      0.01      0.02      7845\n",
      "         14       0.28      0.44      0.34     52125\n",
      "         15       0.23      0.01      0.03     14452\n",
      "         16       0.28      0.46      0.35     52120\n",
      "         17       0.18      0.01      0.03      9547\n",
      "         18       0.27      0.16      0.20     28995\n",
      "         19       0.13      0.02      0.03      3803\n",
      "         20       0.30      0.31      0.30     16014\n",
      "\n",
      "avg / total       0.25      0.26      0.23    252494\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jishe/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# print(confusion_matrix(y_test, preds))\n",
    "# print('\\n')\n",
    "print(classification_report(y_dev, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this is not a very well-predicting model (what the scoring matrix mean? http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html ). But again this is a very rough model. More modifications (e.g., bigram, tagging, stemming) on the input data can be done to possibly get the X matrix \"cleaner\" and better modeling options can be examined. I'll explore more next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Gap\n",
    "Gap module tryout (more details refer to https://github.com/andrewferlitsch/Gap). Prepare text data into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jishe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jishe/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# import Document and Page from the document module# impor \n",
    "from splitter import Document, Page\n",
    "# import the Words class\n",
    "from syntax import Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'good', 'tag': 0}, {'word': 'unique', 'tag': 0}, {'word': 'take', 'tag': 0}, {'word': 'on', 'tag': 0}, {'word': 'the', 'tag': 0}, {'word': 'hidden', 'tag': 0}, {'word': 'role', 'tag': 0}, {'word': 'games', 'tag': 0}, {'word': 'the', 'tag': 0}, {'word': 'good', 'tag': 0}, {'word': 'and', 'tag': 0}, {'word': 'evil', 'tag': 0}, {'word': 'team', 'tag': 0}, {'word': 'win', 'tag': 0}, {'word': 'if', 'tag': 0}, {'word': 'they', 'tag': 0}, {'word': 'eliminate', 'tag': 0}, {'word': 'each', 'tag': 0}, {'word': 'other', 'tag': 0}, {'word': 'where', 'tag': 0}, {'word': 'the', 'tag': 0}, {'word': 'neutral', 'tag': 0}, {'word': 'team', 'tag': 0}, {'word': 'has', 'tag': 0}, {'word': 'unique', 'tag': 0}, {'word': 'objectives', 'tag': 0}, {'word': 'depending', 'tag': 0}, {'word': 'on', 'tag': 0}, {'word': 'what', 'tag': 0}, {'word': 'was', 'tag': 0}, {'word': 'dealt', 'tag': 0}, {'word': 'bad', 'tag': 0}, {'word': 'component', 'tag': 0}, {'word': 'quality', 'tag': 0}, {'word': 'is', 'tag': 0}, {'word': 'on', 'tag': 0}, {'word': 'the', 'tag': 0}, {'word': 'lower', 'tag': 0}, {'word': 'end', 'tag': 0}, {'word': 'the', 'tag': 0}, {'word': 'artwork', 'tag': 0}, {'word': 'is', 'tag': 0}, {'word': 'lackluster', 'tag': 0}, {'word': 'the', 'tag': 0}, {'word': 'card', 'tag': 0}, {'word': 'stock', 'tag': 0}, {'word': 'is', 'tag': 0}, {'word': 'flimsy', 'tag': 0}, {'word': 'board', 'tag': 0}, {'word': 'design', 'tag': 0}, {'word': 'is', 'tag': 0}, {'word': 'uninspired', 'tag': 0}, {'word': 'playing', 'tag': 0}, {'word': 'this', 'tag': 0}, {'word': 'is', 'tag': 0}, {'word': 'entertaining', 'tag': 0}, {'word': 'but', 'tag': 0}, {'word': 'its', 'tag': 0}, {'word': 'tough', 'tag': 0}, {'word': 'getting', 'tag': 0}, {'word': 'folks', 'tag': 0}, {'word': 'interested', 'tag': 0}, {'word': 'when', 'tag': 0}, {'word': 'newer', 'tag': 0}, {'word': 'games', 'tag': 0}, {'word': 'have', 'tag': 0}, {'word': 'a', 'tag': 0}, {'word': 'much', 'tag': 0}, {'word': 'better', 'tag': 0}, {'word': 'graphical', 'tag': 0}, {'word': 'presentation', 'tag': 0}]\n"
     ]
    }
   ],
   "source": [
    "# Process this well-known typing phrase which contains all 26 letters of the alphabet\n",
    "w = Words(reviews['comment'].loc[0], bare=True)\n",
    "print(w.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'a', 'tag': 0}, {'word': 'neat', 'tag': 0}, {'word': 'social', 'tag': 0}, {'word': 'deduction', 'tag': 0}, {'word': 'game', 'tag': 0}, {'word': 'with', 'tag': 0}, {'word': 'multiple', 'tag': 0}, {'word': 'teams', 'tag': 0}, {'word': 'and', 'tag': 0}, {'word': 'winning', 'tag': 0}, {'word': 'conditions', 'tag': 0}, {'word': 'happening', 'tag': 0}, {'word': 'at', 'tag': 0}, {'word': 'the', 'tag': 0}, {'word': 'same', 'tag': 0}, {'word': 'time', 'tag': 0}]\n",
      "[{'word': 'neat', 'tag': 0}, {'word': 'social', 'tag': 0}, {'word': 'deduct', 'tag': 0}, {'word': 'game', 'tag': 0}, {'word': 'multipl', 'tag': 0}, {'word': 'team', 'tag': 0}, {'word': 'win', 'tag': 0}, {'word': 'condit', 'tag': 0}, {'word': 'happen', 'tag': 0}, {'word': 'time', 'tag': 0}]\n",
      "[{'word': 'neat', 'tag': 0}, {'word': 'social', 'tag': 0}, {'word': 'deduction', 'tag': 0}, {'word': 'game', 'tag': 0}, {'word': 'multiple', 'tag': 0}, {'word': 'team', 'tag': 0}, {'word': 'win', 'tag': 0}, {'word': 'condition', 'tag': 0}, {'word': 'happen', 'tag': 0}, {'word': 'time', 'tag': 0}]\n"
     ]
    }
   ],
   "source": [
    "# Process this well-known typing phrase which contains all 26 letters of the alphabet\n",
    "num=1\n",
    "w = Words(reviews['comment'].loc[num], bare=True)\n",
    "print(w.words)\n",
    "\n",
    "w = Words(reviews['comment'].loc[num], stem='porter')\n",
    "print(w.words)\n",
    "\n",
    "w = Words(reviews['comment'].loc[num], stem='gap')\n",
    "print(w.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function (from Andrew) to make it easier to display preprocessed text w/o tagging\n",
    "def towords(words):\n",
    "    for word in words:\n",
    "        print(word['word'], ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neat  \n",
      "social  \n",
      "deduction  \n",
      "game  \n",
      "multiple  \n",
      "team  \n",
      "win  \n",
      "condition  \n",
      "happen  \n",
      "time  \n"
     ]
    }
   ],
   "source": [
    "towords(w.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just some simple manipulation based on Andrew's code. A lot of pruning work needed. More details come later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
